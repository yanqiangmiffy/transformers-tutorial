# transformers-tutorial

> huggingface transformers教程

## 内容列表	

- 00-认识transformers.ipynb：Huggingface `tokenizer；model`
- 01-sentiment-analysis-with-bert：情感分类 `分类任务`
- 02-Guide to HuggingFace Schedulers & Differential LRs：学习率指南 `使用技巧`
- 03-企业隐患huggingface baseline:企业隐患文本分类 `分类任务`
- 04-toxic_multilabel.ipynb :言论多标签分类 `多标签分类`
- 05-Fine_tune_ALBERT_sentence_pair_classification：句子对分类 `文本对分类`
- 06-Multimodal_product_classification：多模态分类 `多模态-文本和图像`
- 07-RoBERTa on NSP using Trainer：NSP预训练 `预训练-Next Sentence Prediction`
- 08-utilizing-transformer-representations-efficiently.ipynb：不同层的使用 `微调-layers`
- 09-RoBERTa Base Fine-Tuning with Better Training Strategies：不同学习率使用 `微调-learning rate`
- 10-pytorch-roberta-pretrain.ipynb：预训练模型继续训练 `预训练-ITPT`
- 11-best-transformer-representations.ipynb：不同层的使用 `微调-layers`
- 12-How to Fine-Tune BERT for Text Classification.ipynb：文本分类上分技巧：`论文`
- 13-Custom_Named_Entity_Recognition_with_BERT.ipynb：命名实体识别：`NER`
- 14-Multiple_choice_on_SWAG：多项选择：`MRC`
- 15-Translation.ipynb：机器翻译：`NMT`
- 16-Summarization.ipynb：文本摘要：`text summary`
- 17-bertviz_examples.ipynb：Bert可视化：`Bert可视化`
- 18-IndicBART and IndicNLG.ipynb：Bart继续预训练：`Bart继续预训练`
## 参考资料

- https://github.com/NadirEM/nlp-notebooks
