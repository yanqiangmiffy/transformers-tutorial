{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## RoBERTa Base Fine-Tuning with Better Training Strategies\n\n### Steps:\n1. [RoBERTa Pretraining](https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-itpt) on Task Data\n2. [RoBERTa Finetuing]() - This Notebook.\n3. [RoBERTa Inference](https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-infer) with weights available.\n\n\n### What's New?\n1. [SWA, Apex AMP & Interpreting Transformers in Torch](https://www.kaggle.com/rhtsingh/swa-apex-amp-interpreting-transformers-in-torch) notebook is an implementation of the Stochastic Weight Averaging technique with NVIDIA Apex on transformers using PyTorch. The notebook also implements how to interactively interpret Transformers using LIT (Language Interpretability Tool) a platform for NLP model understanding.   \nIt has in-depth explanations and code implementations for,\n - SWA \n - Apex AMP\n - Weighted Layer Pooling\n - MADGRAD Optimizer\n - Grouped LLRD\n - Language Interpretibility Tool\n    - Attention Visualization\n    - Saliency Maps\n    - Integrated Gradients\n    - LIME \n    - Embedding Space (UMAP & PCA)\n    - Counterfactual generation\n    - And many more ...\n\n\n2. [Utilizing Transformer Representations Efficiently](https://www.kaggle.com/rhtsingh/utilizing-transformer-representations-efficiently) notebook will show many different ways these outputs and hidden representations can be utilized to do much more than just adding an output layer. It has code implementations and detailed explanations for all the below techniques,\n - Pooler Output  \n - Last Hidden State Output  \n    - CLS Embeddings  \n    - Mean Pooling  \n    - Max Pooling  \n    - Mean + Max Pooling  \n    - Conv1D Pooling  \n - Hidden Layers Output  \n    - Layerwise CLS Embeddings  \n    - Concatenate Pooling  \n    - Weighted Layer Pooling  \n    - LSTM / GRU Pooling  \n    - Attention Pooling  \n    - WKPooling  \n\n3. [On Stability of Few-Sample Transformer Fine-Tuning](https://www.kaggle.com/rhtsingh/on-stability-of-few-sample-transformer-fine-tuning) notebook goes over various remedies to increase few-sample fine-tuning stability and they show a significant performance improvement over simple finetuning methods. The methods explained in the notebook are - \n - Debiasing Omission In BertADAM\n - Re-Initializing Transformer Layers\n - Utilizing Intermediate Layers\n - Layer-wise Learning Rate Decay (LLRD) \n - Mixout Regularization\n - Pre-trained Weight Decay\n - Stochastic Weight Averaging. \n \n4. [Speeding up Transformer w/ Optimization Strategies](https://www.kaggle.com/rhtsingh/speeding-up-transformer-w-optimization-strategies) notebook explains in-depth 5 optimization strategies with code. All these techniques are promising and can improve the model performance both in terms of speed and accuracy.\n   - Dynamic Padding and Uniform Length Batching\n   - Gradient Accumulation\n   - Freeze Embedding\n   - Numeric Precision Reduction\n   - Gradient Checkpointing  ","metadata":{}},{"cell_type":"markdown","source":"### Load Data","metadata":{"papermill":{"duration":0.017372,"end_time":"2021-05-11T19:26:26.916863","exception":false,"start_time":"2021-05-11T19:26:26.899491","status":"completed"},"tags":[],"id":"vietnamese-nursery"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest = pd.read_csv('../input/commonlitreadabilityprize/test.csv')","metadata":{"id":"PyldEJ_jq4yx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import model_selection\ndef create_folds(data, num_splits):\n    data[\"kfold\"] = -1\n    kf = model_selection.KFold(n_splits=num_splits, shuffle=True, random_state=2021)\n    for f, (t_, v_) in enumerate(kf.split(X=data)):\n        data.loc[v_, 'kfold'] = f\n    return data\ntrain = create_folds(train, num_splits=5)","metadata":{"execution":{"iopub.execute_input":"2021-05-11T19:26:27.117055Z","iopub.status.busy":"2021-05-11T19:26:27.116234Z","iopub.status.idle":"2021-05-11T19:26:27.914773Z","shell.execute_reply":"2021-05-11T19:26:27.913854Z"},"papermill":{"duration":0.828531,"end_time":"2021-05-11T19:26:27.914917","exception":false,"start_time":"2021-05-11T19:26:27.086386","status":"completed"},"tags":[],"id":"nearby-expert"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import Dependencies - Modelling","metadata":{"papermill":{"duration":0.017397,"end_time":"2021-05-11T19:26:27.950181","exception":false,"start_time":"2021-05-11T19:26:27.932784","status":"completed"},"tags":[],"id":"legal-float"}},{"cell_type":"code","source":"%matplotlib inline\nfrom glob import glob\nimport os\nimport matplotlib.pyplot as plt\nimport json\nfrom collections import defaultdict\nimport gc\ngc.enable()","metadata":{"execution":{"iopub.execute_input":"2021-05-11T19:26:27.992516Z","iopub.status.busy":"2021-05-11T19:26:27.991711Z","iopub.status.idle":"2021-05-11T19:26:27.994184Z","shell.execute_reply":"2021-05-11T19:26:27.993779Z"},"papermill":{"duration":0.026039,"end_time":"2021-05-11T19:26:27.994289","exception":false,"start_time":"2021-05-11T19:26:27.96825","status":"completed"},"tags":[],"id":"complete-breakdown"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.optimizer import Optimizer\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom torch.utils.data import (\n    Dataset, DataLoader, \n    SequentialSampler, RandomSampler\n)\nfrom transformers import AutoConfig\nfrom transformers import (\n    get_cosine_schedule_with_warmup, \n    get_cosine_with_hard_restarts_schedule_with_warmup,\n    get_linear_schedule_with_warmup\n)\nfrom transformers import AdamW\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom transformers import MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING\nfrom IPython.display import clear_output\nfrom tqdm import tqdm, trange","metadata":{"papermill":{"duration":7.228235,"end_time":"2021-05-11T19:26:35.239965","exception":false,"start_time":"2021-05-11T19:26:28.01173","status":"completed"},"tags":[],"id":"willing-affairs","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Convert Examples to Features","metadata":{"papermill":{"duration":0.020552,"end_time":"2021-05-11T19:26:35.281616","exception":false,"start_time":"2021-05-11T19:26:35.261064","status":"completed"},"tags":[],"id":"palestinian-immunology"}},{"cell_type":"code","source":"def convert_examples_to_features(data, tokenizer, max_len, is_test=False):\n    data = data.replace('\\n', '')\n    tok = tokenizer.encode_plus(\n        data, \n        max_length=max_len, \n        truncation=True,\n        return_attention_mask=True,\n        return_token_type_ids=True\n    )\n    curr_sent = {}\n    padding_length = max_len - len(tok['input_ids'])\n    curr_sent['input_ids'] = tok['input_ids'] + ([tokenizer.pad_token_id] * padding_length)\n    curr_sent['token_type_ids'] = tok['token_type_ids'] + \\\n        ([0] * padding_length)\n    curr_sent['attention_mask'] = tok['attention_mask'] + \\\n        ([0] * padding_length)\n    return curr_sent","metadata":{"execution":{"iopub.execute_input":"2021-05-11T19:26:35.328454Z","iopub.status.busy":"2021-05-11T19:26:35.32758Z","iopub.status.idle":"2021-05-11T19:26:35.329972Z","shell.execute_reply":"2021-05-11T19:26:35.330375Z"},"papermill":{"duration":0.028976,"end_time":"2021-05-11T19:26:35.330531","exception":false,"start_time":"2021-05-11T19:26:35.301555","status":"completed"},"tags":[],"id":"irish-fossil"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset Retriever","metadata":{"papermill":{"duration":0.019304,"end_time":"2021-05-11T19:26:35.369266","exception":false,"start_time":"2021-05-11T19:26:35.349962","status":"completed"},"tags":[],"id":"subject-entertainment"}},{"cell_type":"code","source":"class DatasetRetriever(Dataset):\n    def __init__(self, data, tokenizer, max_len, is_test=False):\n        self.data = data\n        if 'excerpt' in self.data.columns:\n            self.excerpts = self.data.excerpt.values.tolist()\n        else:\n            self.excerpts = self.data.text.values.tolist()\n        self.targets = self.data.target.values.tolist()\n        self.tokenizer = tokenizer\n        self.is_test = is_test\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, item):\n        excerpt, label = self.excerpts[item], self.targets[item]\n        features = convert_examples_to_features(\n            excerpt, self.tokenizer, \n            self.max_len, self.is_test\n        )\n        return {\n            'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n            'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n            'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n            'label':torch.tensor(label, dtype=torch.double),\n        }","metadata":{"execution":{"iopub.execute_input":"2021-05-11T19:26:35.41671Z","iopub.status.busy":"2021-05-11T19:26:35.415851Z","iopub.status.idle":"2021-05-11T19:26:35.418118Z","shell.execute_reply":"2021-05-11T19:26:35.41868Z"},"papermill":{"duration":0.029976,"end_time":"2021-05-11T19:26:35.418819","exception":false,"start_time":"2021-05-11T19:26:35.388843","status":"completed"},"tags":[],"id":"solar-group"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{"papermill":{"duration":0.01885,"end_time":"2021-05-11T19:26:35.456835","exception":false,"start_time":"2021-05-11T19:26:35.437985","status":"completed"},"tags":[],"id":"proprietary-genre"}},{"cell_type":"code","source":"class CommonLitModel(nn.Module):\n    def __init__(\n        self, \n        model_name, \n        config,  \n        multisample_dropout=False,\n        output_hidden_states=False\n    ):\n        super(CommonLitModel, self).__init__()\n        self.config = config\n        self.roberta = AutoModel.from_pretrained(\n            model_name, \n            output_hidden_states=output_hidden_states\n        )\n        self.layer_norm = nn.LayerNorm(config.hidden_size)\n        if multisample_dropout:\n            self.dropouts = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        #self.regressor = nn.Linear(config.hidden_size*2, 1)\n        self.regressor = nn.Linear(config.hidden_size, 1)\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.regressor)\n \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n \n    def forward(\n        self, \n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        labels=None\n    ):\n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n        )\n        sequence_output = outputs[1]\n        sequence_output = self.layer_norm(sequence_output)\n \n        # max-avg head\n        # average_pool = torch.mean(sequence_output, 1)\n        # max_pool, _ = torch.max(sequence_output, 1)\n        # concat_sequence_output = torch.cat((average_pool, max_pool), 1)\n \n        # multi-sample dropout\n        for i, dropout in enumerate(self.dropouts):\n            if i == 0:\n                logits = self.regressor(dropout(sequence_output))\n            else:\n                logits += self.regressor(dropout(sequence_output))\n        \n        logits /= len(self.dropouts)\n \n        # calculate loss\n        loss = None\n        if labels is not None:\n            # regression task\n            loss_fn = torch.nn.MSELoss()\n            logits = logits.view(-1).to(labels.dtype)\n            loss = torch.sqrt(loss_fn(logits, labels.view(-1)))\n        \n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output","metadata":{"execution":{"iopub.execute_input":"2021-05-11T19:26:35.51111Z","iopub.status.busy":"2021-05-11T19:26:35.510248Z","iopub.status.idle":"2021-05-11T19:26:35.512883Z","shell.execute_reply":"2021-05-11T19:26:35.512449Z"},"papermill":{"duration":0.037145,"end_time":"2021-05-11T19:26:35.512991","exception":false,"start_time":"2021-05-11T19:26:35.475846","status":"completed"},"tags":[],"id":"resident-kazakhstan"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lamb Optimizer","metadata":{"papermill":{"duration":0.019467,"end_time":"2021-05-11T19:26:35.552336","exception":false,"start_time":"2021-05-11T19:26:35.532869","status":"completed"},"tags":[],"id":"going-tractor"}},{"cell_type":"code","source":"class Lamb(Optimizer):\n    # Reference code: https://github.com/cybertronai/pytorch-lamb\n\n    def __init__(\n        self,\n        params,\n        lr: float = 1e-3,\n        betas = (0.9, 0.999),\n        eps: float = 1e-6,\n        weight_decay: float = 0,\n        clamp_value: float = 10,\n        adam: bool = False,\n        debias: bool = False,\n    ):\n        if lr <= 0.0:\n            raise ValueError('Invalid learning rate: {}'.format(lr))\n        if eps < 0.0:\n            raise ValueError('Invalid epsilon value: {}'.format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\n                'Invalid beta parameter at index 0: {}'.format(betas[0])\n            )\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\n                'Invalid beta parameter at index 1: {}'.format(betas[1])\n            )\n        if weight_decay < 0:\n            raise ValueError(\n                'Invalid weight_decay value: {}'.format(weight_decay)\n            )\n        if clamp_value < 0.0:\n            raise ValueError('Invalid clamp value: {}'.format(clamp_value))\n\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        self.clamp_value = clamp_value\n        self.adam = adam\n        self.debias = debias\n\n        super(Lamb, self).__init__(params, defaults)\n\n    def step(self, closure = None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    msg = (\n                        'Lamb does not support sparse gradients, '\n                        'please consider SparseAdam instead'\n                    )\n                    raise RuntimeError(msg)\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(\n                        p, memory_format=torch.preserve_format\n                    )\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(\n                        p, memory_format=torch.preserve_format\n                    )\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                state['step'] += 1\n\n                # Decay the first and second moment running average coefficient\n                # m_t\n                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n                # v_t\n                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n\n                # Paper v3 does not use debiasing.\n                if self.debias:\n                    bias_correction = math.sqrt(1 - beta2 ** state['step'])\n                    bias_correction /= 1 - beta1 ** state['step']\n                else:\n                    bias_correction = 1\n\n                # Apply bias to lr to avoid broadcast.\n                step_size = group['lr'] * bias_correction\n\n                weight_norm = torch.norm(p.data).clamp(0, self.clamp_value)\n\n                adam_step = exp_avg / exp_avg_sq.sqrt().add(group['eps'])\n                if group['weight_decay'] != 0:\n                    adam_step.add_(p.data, alpha=group['weight_decay'])\n\n                adam_norm = torch.norm(adam_step)\n                if weight_norm == 0 or adam_norm == 0:\n                    trust_ratio = 1\n                else:\n                    trust_ratio = weight_norm / adam_norm\n                state['weight_norm'] = weight_norm\n                state['adam_norm'] = adam_norm\n                state['trust_ratio'] = trust_ratio\n                if self.adam:\n                    trust_ratio = 1\n\n                p.data.add_(adam_step, alpha=-step_size * trust_ratio)\n\n        return loss","metadata":{"execution":{"iopub.execute_input":"2021-05-11T19:26:35.608787Z","iopub.status.busy":"2021-05-11T19:26:35.608039Z","iopub.status.idle":"2021-05-11T19:26:35.610861Z","shell.execute_reply":"2021-05-11T19:26:35.610406Z"},"papermill":{"duration":0.039162,"end_time":"2021-05-11T19:26:35.610963","exception":false,"start_time":"2021-05-11T19:26:35.571801","status":"completed"},"tags":[],"id":"photographic-crack"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Differential Learning Rate and Weight Decay","metadata":{"papermill":{"duration":0.019229,"end_time":"2021-05-11T19:26:35.648664","exception":false,"start_time":"2021-05-11T19:26:35.629435","status":"completed"},"tags":[],"id":"exceptional-asbestos"}},{"cell_type":"code","source":"def get_optimizer_params(model):\n    # differential learning rate and weight decay\n    param_optimizer = list(model.named_parameters())\n    learning_rate = 5e-5\n    no_decay = ['bias', 'gamma', 'beta']\n    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n    optimizer_parameters = [\n        {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.01},\n        {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.01, 'lr': learning_rate/2.6},\n        {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.01, 'lr': learning_rate},\n        {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.01, 'lr': learning_rate*2.6},\n        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': learning_rate/2.6},\n        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': learning_rate},\n        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': learning_rate*2.6},\n        {'params': [p for n, p in model.named_parameters() if \"roberta\" not in n], 'lr':1e-3, \"momentum\" : 0.99},\n    ]\n    return optimizer_parameters","metadata":{"execution":{"iopub.execute_input":"2021-05-11T19:26:35.700523Z","iopub.status.busy":"2021-05-11T19:26:35.69983Z","iopub.status.idle":"2021-05-11T19:26:35.702594Z","shell.execute_reply":"2021-05-11T19:26:35.702187Z"},"papermill":{"duration":0.034924,"end_time":"2021-05-11T19:26:35.702699","exception":false,"start_time":"2021-05-11T19:26:35.667775","status":"completed"},"tags":[],"id":"worth-distance"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Utilities","metadata":{"papermill":{"duration":0.018135,"end_time":"2021-05-11T19:26:35.739429","exception":false,"start_time":"2021-05-11T19:26:35.721294","status":"completed"},"tags":[],"id":"respiratory-improvement"}},{"cell_type":"code","source":"def make_model(model_name='../content/roberta-base-5-epochs/', num_labels=1):\n    tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n    config = AutoConfig.from_pretrained(model_name)\n    config.update({'num_labels':num_labels})\n    model = CommonLitModel(model_name, config=config)\n    return model, tokenizer\n\ndef make_optimizer(model, optimizer_name=\"AdamW\"):\n    optimizer_grouped_parameters = get_optimizer_params(model)\n    kwargs = {\n            'lr':5e-5,\n            'weight_decay':0.01,\n            # 'betas': (0.9, 0.98),\n            # 'eps': 1e-06\n    }\n    if optimizer_name == \"LAMB\":\n        optimizer = Lamb(optimizer_grouped_parameters, **kwargs)\n        return optimizer\n    elif optimizer_name == \"Adam\":\n        from torch.optim import Adam\n        optimizer = Adam(optimizer_grouped_parameters, **kwargs)\n        return optimizer\n    elif optimizer_name == \"AdamW\":\n        optimizer = AdamW(optimizer_grouped_parameters, **kwargs)\n        return optimizer\n    else:\n        raise Exception('Unknown optimizer: {}'.format(optimizer_name))\n\ndef make_scheduler(optimizer, decay_name='linear', t_max=None, warmup_steps=None):\n    if decay_name == 'step':\n        scheduler = optim.lr_scheduler.MultiStepLR(\n            optimizer,\n            milestones=[30, 60, 90],\n            gamma=0.1\n        )\n    elif decay_name == 'cosine':\n        scheduler = lrs.CosineAnnealingLR(\n            optimizer,\n            T_max=t_max\n        )\n    elif decay_name == \"cosine_warmup\":\n        scheduler = get_cosine_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=warmup_steps,\n            num_training_steps=t_max\n        )\n    elif decay_name == \"linear\":\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer, \n            num_warmup_steps=warmup_steps, \n            num_training_steps=t_max\n        )\n    else:\n        raise Exception('Unknown lr scheduler: {}'.format(decay_type))    \n    return scheduler    \n\ndef make_loader(\n    data, \n    tokenizer, \n    max_len,\n    batch_size,\n    fold=0\n):\n    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n    train_dataset = DatasetRetriever(train_set, tokenizer, max_len)\n    valid_dataset = DatasetRetriever(valid_set, tokenizer, max_len)\n\n    train_sampler = RandomSampler(train_dataset)\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=batch_size, \n        sampler=train_sampler, \n        pin_memory=True, \n        drop_last=False, \n        num_workers=4\n    )\n\n    valid_sampler = SequentialSampler(valid_dataset)\n    valid_loader = DataLoader(\n        valid_dataset, \n        batch_size=batch_size // 2, \n        sampler=valid_sampler, \n        pin_memory=True, \n        drop_last=False, \n        num_workers=4\n    )\n\n    return train_loader, valid_loader","metadata":{"execution":{"iopub.execute_input":"2021-05-11T19:26:35.788703Z","iopub.status.busy":"2021-05-11T19:26:35.787892Z","iopub.status.idle":"2021-05-11T19:26:35.79056Z","shell.execute_reply":"2021-05-11T19:26:35.790141Z"},"papermill":{"duration":0.032912,"end_time":"2021-05-11T19:26:35.790678","exception":false,"start_time":"2021-05-11T19:26:35.757766","status":"completed"},"tags":[],"id":"ahead-steering"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Metrics","metadata":{"papermill":{"duration":0.018561,"end_time":"2021-05-11T19:26:35.827595","exception":false,"start_time":"2021-05-11T19:26:35.809034","status":"completed"},"tags":[],"id":"written-resort"}},{"cell_type":"code","source":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n        self.max = 0\n        self.min = 1e5\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n        if val > self.max:\n            self.max = val\n        if val < self.min:\n            self.min = val","metadata":{"execution":{"iopub.execute_input":"2021-05-11T19:26:35.870815Z","iopub.status.busy":"2021-05-11T19:26:35.870145Z","iopub.status.idle":"2021-05-11T19:26:35.872923Z","shell.execute_reply":"2021-05-11T19:26:35.872518Z"},"papermill":{"duration":0.027034,"end_time":"2021-05-11T19:26:35.873026","exception":false,"start_time":"2021-05-11T19:26:35.845992","status":"completed"},"tags":[],"id":"changed-monte"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Trainer","metadata":{"papermill":{"duration":0.01885,"end_time":"2021-05-11T19:26:35.910429","exception":false,"start_time":"2021-05-11T19:26:35.891579","status":"completed"},"tags":[],"id":"atmospheric-correspondence"}},{"cell_type":"code","source":"class Trainer:\n    def __init__(self, model, optimizer, scheduler, scalar=None, log_interval=1, evaluate_interval=1):\n        self.model = model\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.scalar = scalar\n        self.log_interval = log_interval\n        self.evaluate_interval = evaluate_interval\n        self.evaluator = Evaluator(self.model, self.scalar)\n\n    def train(self, train_loader, valid_loader, epoch, \n              result_dict, tokenizer, fold):\n        count = 0\n        losses = AverageMeter()\n        self.model.train()\n        \n        for batch_idx, batch_data in enumerate(train_loader):\n            input_ids, attention_mask, token_type_ids, labels = batch_data['input_ids'], \\\n                batch_data['attention_mask'], batch_data['token_type_ids'], batch_data['label']\n            input_ids, attention_mask, token_type_ids, labels = \\\n                input_ids.cuda(), attention_mask.cuda(), token_type_ids.cuda(), labels.cuda()\n            \n            if self.scalar is not None:\n                with torch.cuda.amp.autocast():\n                    outputs = self.model(\n                        input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        token_type_ids=token_type_ids,\n                        labels=labels\n                    )\n            else:\n                outputs = self.model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    token_type_ids=token_type_ids,\n                    labels=labels\n                )\n\n            loss, logits = outputs[:2]\n            count += labels.size(0)\n            losses.update(loss.item(), input_ids.size(0))\n            \n            if self.scalar is not None:\n                self.scalar.scale(loss).backward()\n                self.scalar.step(self.optimizer)\n                self.scalar.update()\n            else:\n                loss.backward()\n                self.optimizer.step()\n\n            self.scheduler.step()\n            self.optimizer.zero_grad()\n\n            if batch_idx % self.log_interval == 0:\n                _s = str(len(str(len(train_loader.sampler))))\n                ret = [\n                    ('epoch: {:0>3} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(epoch, count, len(train_loader.sampler), 100 * count / len(train_loader.sampler)),\n                    'train_loss: {: >4.5f}'.format(losses.avg),\n                ]\n                print(', '.join(ret))\n            \n            if batch_idx % self.evaluate_interval == 0:\n                result_dict = self.evaluator.evaluate(\n                    valid_loader, \n                    epoch, \n                    result_dict, \n                    tokenizer\n                )\n                if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n                    print(\"{} epoch, best epoch was updated! valid_loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n                    result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]\n                    torch.save(self.model.state_dict(), f\"model{fold}.bin\")\n\n        result_dict['train_loss'].append(losses.avg)\n        return result_dict","metadata":{"execution":{"iopub.execute_input":"2021-05-11T19:26:35.960435Z","iopub.status.busy":"2021-05-11T19:26:35.959727Z","iopub.status.idle":"2021-05-11T19:26:35.962459Z","shell.execute_reply":"2021-05-11T19:26:35.962067Z"},"papermill":{"duration":0.033486,"end_time":"2021-05-11T19:26:35.962579","exception":false,"start_time":"2021-05-11T19:26:35.929093","status":"completed"},"tags":[],"id":"chubby-liberty"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluator","metadata":{"papermill":{"duration":0.019241,"end_time":"2021-05-11T19:26:36.000319","exception":false,"start_time":"2021-05-11T19:26:35.981078","status":"completed"},"tags":[],"id":"interested-glass"}},{"cell_type":"code","source":"class Evaluator:\n    def __init__(self, model, scalar=None):\n        self.model = model\n        self.scalar = scalar\n    \n    def worst_result(self):\n        ret = {\n            'loss':float('inf'),\n            'accuracy':0.0\n        }\n        return ret\n\n    def result_to_str(self, result):\n        ret = [\n            'epoch: {epoch:0>3}',\n            'loss: {loss: >4.2e}'\n        ]\n        for metric in self.evaluation_metrics:\n            ret.append('{}: {}'.format(metric.name, metric.fmtstr))\n        return ', '.join(ret).format(**result)\n\n    def save(self, result):\n        with open('result_dict.json', 'w') as f:\n            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n    \n    def load(self):\n        result = self.worst_result\n        if os.path.exists('result_dict.json'):\n            with open('result_dict.json', 'r') as f:\n                try:\n                    result = json.loads(f.read())\n                except:\n                    pass\n        return result\n\n    def evaluate(self, data_loader, epoch, result_dict, tokenizer):\n        losses = AverageMeter()\n\n        self.model.eval()\n        total_loss = 0\n        with torch.no_grad():\n            for batch_idx, batch_data in enumerate(data_loader):\n                input_ids, attention_mask, token_type_ids, labels = batch_data['input_ids'], \\\n                    batch_data['attention_mask'], batch_data['token_type_ids'], batch_data['label']\n                input_ids, attention_mask, token_type_ids, labels = input_ids.cuda(), \\\n                    attention_mask.cuda(), token_type_ids.cuda(), labels.cuda()\n                \n                if self.scalar is not None:\n                    with torch.cuda.amp.autocast():\n                        outputs = self.model(\n                            input_ids=input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            labels=labels\n                        )\n                else:\n                    outputs = self.model(\n                        input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        token_type_ids=token_type_ids,\n                        labels=labels\n                    )\n                \n                loss, logits = outputs[:2]\n                losses.update(loss.item(), input_ids.size(0))\n\n        print('----Validation Results Summary----')\n        print('Epoch: [{}] valid_loss: {: >4.5f}'.format(epoch, losses.avg))\n\n        result_dict['val_loss'].append(losses.avg)        \n        return result_dict","metadata":{"execution":{"iopub.execute_input":"2021-05-11T19:26:36.052351Z","iopub.status.busy":"2021-05-11T19:26:36.05141Z","iopub.status.idle":"2021-05-11T19:26:36.054072Z","shell.execute_reply":"2021-05-11T19:26:36.053643Z"},"papermill":{"duration":0.034578,"end_time":"2021-05-11T19:26:36.054175","exception":false,"start_time":"2021-05-11T19:26:36.019597","status":"completed"},"tags":[],"id":"demonstrated-delhi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Config","metadata":{"papermill":{"duration":0.018509,"end_time":"2021-05-11T19:26:36.091323","exception":false,"start_time":"2021-05-11T19:26:36.072814","status":"completed"},"tags":[],"id":"solved-ownership"}},{"cell_type":"code","source":"def config(fold=0):\n    torch.manual_seed(2021)\n    torch.cuda.manual_seed(2021)\n    torch.cuda.manual_seed_all(2021)\n    epochs = 8\n    max_len = 250\n    batch_size = 16\n\n    model, tokenizer = make_model(model_name='../content/roberta-base-5-epochs/', num_labels=1)\n    train_loader, valid_loader = make_loader(\n        train, tokenizer, max_len=max_len,\n        batch_size=batch_size, fold=fold\n    )\n\n    import math\n    num_update_steps_per_epoch = len(train_loader)\n    max_train_steps = epochs * num_update_steps_per_epoch\n    warmup_proportion = 0\n    if warmup_proportion != 0:\n        warmup_steps = math.ceil((max_train_steps * 2) / 100)\n    else:\n        warmup_steps = 0\n\n    optimizer = make_optimizer(model, \"AdamW\")\n    scheduler = make_scheduler(\n        optimizer, decay_name='cosine_warmup', \n        t_max=max_train_steps, \n        warmup_steps=warmup_steps\n    )    \n\n    if torch.cuda.device_count() >= 1:\n        print('Model pushed to {} GPU(s), type {}.'.format(\n            torch.cuda.device_count(), \n            torch.cuda.get_device_name(0))\n        )\n        model = model.cuda() \n    else:\n        raise ValueError('CPU training is not supported')\n\n    # scaler = torch.cuda.amp.GradScaler()\n    scaler = None\n\n    result_dict = {\n        'epoch':[], \n        'train_loss': [], \n        'val_loss' : [], \n        'best_val_loss': np.inf\n    }\n    return (\n        model, tokenizer, \n        optimizer, scheduler, \n        scaler, train_loader, \n        valid_loader, result_dict, \n        epochs\n    )","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2021-05-11T19:26:36.136292Z","iopub.status.busy":"2021-05-11T19:26:36.135577Z","iopub.status.idle":"2021-05-11T19:26:36.137828Z","shell.execute_reply":"2021-05-11T19:26:36.138253Z"},"papermill":{"duration":0.028282,"end_time":"2021-05-11T19:26:36.138369","exception":false,"start_time":"2021-05-11T19:26:36.110087","status":"completed"},"tags":[],"id":"addressed-function"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Run","metadata":{"papermill":{"duration":0.020208,"end_time":"2021-05-11T19:26:36.177987","exception":false,"start_time":"2021-05-11T19:26:36.157779","status":"completed"},"tags":[],"id":"nonprofit-causing"}},{"cell_type":"code","source":"def run(fold=0):\n    model, tokenizer, optimizer, scheduler, scaler, \\\n        train_loader, valid_loader, result_dict, epochs = config(fold)\n    \n    import time\n    trainer = Trainer(model, optimizer, scheduler, scaler)\n    train_time_list = []\n\n    for epoch in range(epochs):\n        result_dict['epoch'] = epoch\n\n        torch.cuda.synchronize()\n        tic1 = time.time()\n\n        result_dict = trainer.train(train_loader, valid_loader, epoch, \n                                    result_dict, tokenizer, fold)\n\n        torch.cuda.synchronize()\n        tic2 = time.time() \n        train_time_list.append(tic2 - tic1)\n\n    torch.cuda.empty_cache()\n    del model, tokenizer, optimizer, scheduler, \\\n        scaler, train_loader, valid_loader,\n    gc.collect()\n    return result_dict","metadata":{"execution":{"iopub.execute_input":"2021-05-11T19:26:36.227481Z","iopub.status.busy":"2021-05-11T19:26:36.226678Z","iopub.status.idle":"2021-05-11T19:26:36.229314Z","shell.execute_reply":"2021-05-11T19:26:36.228849Z"},"papermill":{"duration":0.032278,"end_time":"2021-05-11T19:26:36.229422","exception":false,"start_time":"2021-05-11T19:26:36.197144","status":"completed"},"tags":[],"id":"suspended-anniversary"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_list = []\nfor fold in range(5):\n    print('----')\n    print(f'FOLD: {fold}')\n    result_dict = run(fold)\n    result_list.append(result_dict)\n    print('----')","metadata":{"execution":{"iopub.execute_input":"2021-05-11T19:26:36.272151Z","iopub.status.busy":"2021-05-11T19:26:36.271631Z","iopub.status.idle":"2021-05-11T20:15:37.136995Z","shell.execute_reply":"2021-05-11T20:15:37.137733Z"},"papermill":{"duration":2940.88873,"end_time":"2021-05-11T20:15:37.137957","exception":false,"start_time":"2021-05-11T19:26:36.249227","status":"completed"},"tags":[],"id":"flush-clause","outputId":"0bb37720-4c50-4662-8128-176e7008e03d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Best Validation Loss per Fold","metadata":{}},{"cell_type":"code","source":"[print(\"FOLD::\", i, \"Loss:: \", fold['best_val_loss']) for i, fold in enumerate(result_list)]","metadata":{"id":"lt2jASMiPH1R","outputId":"473119ae-40c4-4b31-fa9a-b6c42feaf3fb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### OOF Prediction","metadata":{}},{"cell_type":"code","source":"oof = np.zeros(len(train))\nfor fold in tqdm(range(5), total=5):\n    model, tokenizer = make_model()\n    model.load_state_dict(\n        torch.load(f'model{fold}.bin')\n    )\n    model.cuda()\n    model.eval()\n    val_index = train[train.kfold==fold].index.tolist()\n    train_loader, val_loader = make_loader(train, tokenizer, 250, 16, fold=fold)\n    # scalar = torch.cuda.amp.GradScaler()\n    scalar = None\n    preds = []\n    for index, data in enumerate(val_loader):\n        input_ids, attention_mask, token_type_ids, labels = data['input_ids'], \\\n            data['attention_mask'], data['token_type_ids'], data['label']\n        input_ids, attention_mask, token_type_ids, labels = input_ids.cuda(), \\\n            attention_mask.cuda(), token_type_ids.cuda(), labels.cuda()\n        if scalar is not None:\n            with torch.cuda.amp.autocast():\n                outputs = model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    token_type_ids=token_type_ids,\n                    labels=labels\n                )\n        else:\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                token_type_ids=token_type_ids,\n                labels=labels\n            )\n        \n        loss, logits = outputs[:2]\n        preds += logits.cpu().detach().numpy().tolist()\n    oof[val_index] = preds","metadata":{"id":"_aaj35kCjxzT","outputId":"a1830cca-986e-4797-d4ee-06466df3e0d3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Compute Local CV Score","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nround(np.sqrt(mean_squared_error(train.target.values, oof)), 4)","metadata":{"id":"js2Li0srR1Cq","outputId":"bd44fd20-ccf3-4e42-eb81-677648c73105"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}