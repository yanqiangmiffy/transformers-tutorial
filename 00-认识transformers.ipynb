{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig,AutoModel,AutoTokenizer,AdamW,get_linear_schedule_with_warmup,logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset,SequentialSampler,RandomSampler,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预训练模型名称\n",
    "MODEL_NAME=\"bert-base-chinese\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  预训练模型配置\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.6.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer\n",
    "\n",
    "参考文档：https://huggingface.co/transformers/v4.6.0/main_classes/tokenizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='bert-base-chinese', vocab_size=21128, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一些特殊符号：['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 102, 0, 101, 103]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词汇表大小\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将文本转为词汇表id\n",
    "\n",
    "- 方法1\n",
    "```\n",
    "    def encode(\n",
    "        self,\n",
    "        text: Union[TextInput, PreTokenizedInput, EncodedInput],\n",
    "        text_pair: Optional[Union[TextInput, PreTokenizedInput, EncodedInput]] = None,\n",
    "        add_special_tokens: bool = True,\n",
    "        padding: Union[bool, str, PaddingStrategy] = False,\n",
    "        truncation: Union[bool, str, TruncationStrategy] = False,\n",
    "        max_length: Optional[int] = None,\n",
    "        stride: int = 0,\n",
    "        return_tensors: Optional[Union[str, TensorType]] = None,\n",
    "        **kwargs\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\n",
    "\n",
    "        Same as doing ``self.convert_tokens_to_ids(self.tokenize(text))``.\n",
    "\n",
    "        Args:\n",
    "            text (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`):\n",
    "                The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the\n",
    "                ``tokenize`` method) or a list of integers (tokenized string ids using the ``convert_tokens_to_ids``\n",
    "                method).\n",
    "            text_pair (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`, `optional`):\n",
    "                Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
    "                the ``tokenize`` method) or a list of integers (tokenized string ids using the\n",
    "                ``convert_tokens_to_ids`` method).\n",
    "        \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2769, 1762, 1266, 776, 2339, 868, 102]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"我在北京工作\"\n",
    "token_ids=tokenizer.encode(text)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '我', '在', '北', '京', '工', '作', '[SEP]']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将id转为原始字符\n",
    "tokenizer.convert_ids_to_tokens(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "padding的模式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "padding (:obj:`bool`, :obj:`str` or :class:`~transformers.file_utils.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "                 Select a strategy to pad the returned sequences (according to the model's padding side and padding\n",
    "                 index) among:\n",
    "\n",
    "                * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
    "                  single sequence if provided).\n",
    "                * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "                  maximum acceptable input length for the model if that argument is not provided.\n",
    "                * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "                  different lengths).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2769, 1762, 1266, 776, 2339, 868, 102]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加入参数\n",
    "token_ids=tokenizer.encode(text,padding=True,max_length=30,add_special_tokens=True)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2769,\n",
       " 1762,\n",
       " 1266,\n",
       " 776,\n",
       " 2339,\n",
       " 868,\n",
       " 102,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加入参数\n",
    "token_ids=tokenizer.encode(text,padding=\"max_length\",max_length=30,add_special_tokens=True)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 2769, 1762, 1266,  776, 2339,  868,  102,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids=tokenizer.encode(text,padding=\"max_length\",max_length=30,add_special_tokens=True,return_tensors='pt')\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 方法2 encode_plus\n",
    "```\n",
    "def encode_plus(\n",
    "        self,\n",
    "        text: Union[TextInput, PreTokenizedInput, EncodedInput],\n",
    "        text_pair: Optional[Union[TextInput, PreTokenizedInput, EncodedInput]] = None,\n",
    "        add_special_tokens: bool = True,\n",
    "        padding: Union[bool, str, PaddingStrategy] = False,\n",
    "        truncation: Union[bool, str, TruncationStrategy] = False,\n",
    "        max_length: Optional[int] = None,\n",
    "        stride: int = 0,\n",
    "        is_split_into_words: bool = False,\n",
    "        pad_to_multiple_of: Optional[int] = None,\n",
    "        return_tensors: Optional[Union[str, TensorType]] = None,\n",
    "        return_token_type_ids: Optional[bool] = None,\n",
    "        return_attention_mask: Optional[bool] = None,\n",
    "        return_overflowing_tokens: bool = False,\n",
    "        return_special_tokens_mask: bool = False,\n",
    "        return_offsets_mapping: bool = False,\n",
    "        return_length: bool = False,\n",
    "        verbose: bool = True,\n",
    "        **kwargs\n",
    "    ) -> BatchEncoding:\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2769, 1762, 1266,  776, 2339,  868,  102,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids=tokenizer.encode_plus(\n",
    "    text,padding=\"max_length\",\n",
    "    max_length=30,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors='pt',\n",
    "    return_token_type_ids=True,\n",
    "    return_attention_mask=True\n",
    ")\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model=AutoModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs=model(token_ids['input_ids'],token_ids['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1768,  0.2857,  0.0687,  ...,  0.1473,  0.2458, -0.1838],\n",
       "         [-0.1322,  0.2086,  0.6903,  ..., -1.0633, -0.1590,  0.1167],\n",
       "         [-0.6164, -0.0689,  1.3369,  ..., -0.6970,  0.1889,  0.1287],\n",
       "         ...,\n",
       "         [ 0.6877,  0.6504, -0.4347,  ...,  0.8359,  0.0233,  0.3861],\n",
       "         [ 0.5968,  0.6780, -0.5455,  ...,  0.7718,  0.0800,  0.2957],\n",
       "         [ 0.4068,  0.7257, -0.4299,  ...,  0.6403,  0.1024,  0.3176]]],\n",
       "       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[ 0.9856,  0.9988,  0.9976,  0.9687,  0.4322, -0.8034,  0.2726, -0.4748,\n",
       "          0.9974, -0.9965,  1.0000,  0.9998,  0.5398, -0.6888,  0.9998, -0.9807,\n",
       "          0.0292,  0.9986,  0.9717,  0.4810,  0.8733, -1.0000, -0.8902, -0.9659,\n",
       "         -0.6816,  0.9558,  0.9234, -0.9821, -0.9995,  0.9961,  0.9623,  0.9995,\n",
       "          0.9502, -0.9998, -0.9929,  0.8046, -0.1106,  0.9928, -0.4533, -0.9623,\n",
       "         -0.9918, -0.6101, -0.2343, -0.9976, -0.9957,  0.7763, -1.0000, -0.9994,\n",
       "         -0.5729,  0.9878, -0.5281, -0.9999,  0.9717, -0.9702,  0.3639,  0.9965,\n",
       "         -0.9977,  0.9785,  1.0000,  0.7353,  0.9998, -0.8453,  0.0164, -0.9992,\n",
       "          1.0000, -0.9993, -0.9588,  0.0252,  0.9949,  0.9999, -0.9924,  0.7567,\n",
       "          1.0000,  0.6448, -0.3883,  0.9999, -0.9294,  0.3218, -0.9999,  0.7246,\n",
       "          1.0000,  0.9968, -0.9379,  0.7467, -0.9684, -0.9999, -0.9974,  0.9946,\n",
       "         -0.2265,  0.9874,  0.9931, -0.9958, -1.0000,  0.9956, -0.9945, -0.9632,\n",
       "         -0.8202,  0.9864, -0.4922, -0.9166, -0.6189,  0.9722, -0.9995, -0.9989,\n",
       "          0.9989,  0.9971,  0.7095, -0.9954,  0.9979,  0.0965, -1.0000, -0.9232,\n",
       "         -1.0000, -0.6299, -0.9881,  0.9999, -0.5969, -0.2713,  0.9996, -0.9994,\n",
       "          0.9709, -0.9993, -0.7689, -0.5084,  0.9989,  0.9997,  0.9946, -0.9969,\n",
       "          0.9995,  1.0000,  0.9356,  0.9885, -0.9935,  0.9955,  0.6783, -0.9819,\n",
       "         -0.0539, -0.9004,  1.0000,  0.9950,  0.9837, -0.9028,  0.9921, -0.9966,\n",
       "          0.9998, -1.0000,  0.9981, -1.0000, -0.9822,  0.9779,  0.9472,  1.0000,\n",
       "         -0.7185,  0.9997, -0.9868, -0.9896,  0.9909, -0.6951,  0.9794, -0.9998,\n",
       "          0.8378, -0.2424, -0.4286,  0.1382, -1.0000,  1.0000, -0.9185,  0.9999,\n",
       "          0.9997, -0.8165, -0.9952, -0.9933,  0.8819, -0.9958, -0.6735,  0.9903,\n",
       "         -0.6521,  0.9939, -0.8262, -0.9846,  0.7794,  0.4095, -0.9989,  0.9955,\n",
       "         -0.4798, -0.3833, -0.1553,  0.9726,  0.9959,  0.9573, -0.9757,  0.9998,\n",
       "          0.0799,  0.9946,  0.9571,  0.0692, -0.3651, -0.9769, -0.9998,  0.1469,\n",
       "          0.9998, -0.6495, -0.9968,  0.7772, -0.9999,  0.9161,  0.0237,  0.5812,\n",
       "         -0.9922, -0.9998,  0.9997, -0.9761, -0.9942, -0.4493, -0.7579, -0.1070,\n",
       "         -0.9979,  0.0511,  0.9820, -0.9089, -0.4129, -0.9215, -0.9967,  0.9882,\n",
       "         -0.9911,  0.9790,  0.7282,  1.0000,  0.9646, -0.8515, -0.9507,  1.0000,\n",
       "         -0.3611, -1.0000,  0.7958, -0.9529,  0.1575,  0.9999, -0.9987,  0.7636,\n",
       "          1.0000,  0.7107,  1.0000,  0.0779, -0.9992, -0.9956,  1.0000,  0.9989,\n",
       "          0.9998, -0.9513, -0.9602,  0.1727,  0.4385, -1.0000, -0.9801, -0.8604,\n",
       "          0.9979,  0.9998,  0.4816, -0.9992, -0.5944, -0.9991,  0.9998, -0.7465,\n",
       "          1.0000,  0.9979, -0.9676, -0.8473,  0.8218, -0.6071, -0.9996, -0.0390,\n",
       "         -0.9989, -0.9653, -0.9999,  0.8497, -0.9880, -1.0000,  0.5428,  0.9998,\n",
       "          0.8222, -0.9995,  0.9978,  0.9925, -0.9145, -0.9570,  0.9649, -1.0000,\n",
       "          1.0000, -0.9956,  0.7214, -0.9013, -0.9956, -0.4886,  0.9990,  0.9962,\n",
       "         -0.9953, -0.9355, -0.9959, -0.9978, -0.7774,  0.6913, -0.5124,  0.7711,\n",
       "         -0.9819, -0.7130,  0.6875, -0.9596, -0.9998,  0.1750,  0.9999,  0.2758,\n",
       "          1.0000,  0.4193,  1.0000, -0.9497, -0.9626,  0.9851,  0.6160, -0.3189,\n",
       "          0.1307, -0.8954,  0.9028, -0.4798, -0.4572, -0.9988,  0.9996,  0.9796,\n",
       "          0.9792,  0.6918,  0.2685,  0.3946,  0.9280, -0.9834,  0.9990, -0.9981,\n",
       "          0.0669,  0.9998,  0.9997,  0.9991,  0.9276, -0.8520,  0.9163, -0.9988,\n",
       "          0.9974, -0.9957,  0.9971, -0.9547,  0.9215, -0.6406, -0.9869,  1.0000,\n",
       "          0.5743,  0.8119,  0.9996, -0.8633,  0.9257,  0.9894,  0.9300,  0.9830,\n",
       "          0.9185,  0.9995, -0.9984, -0.9986, -0.7267, -0.9952, -0.9984, -1.0000,\n",
       "          0.3547, -0.9696, -0.9820, -0.4348,  0.2577,  0.4939, -0.9808, -0.9613,\n",
       "          0.1973,  0.9320, -0.6785,  0.4800,  0.6760, -0.9969, -0.9770, -0.9999,\n",
       "         -0.9923,  0.6905,  0.9989, -0.9995,  0.9994, -1.0000, -0.1779,  0.6749,\n",
       "         -0.5800, -0.6796,  0.9999, -0.9999,  0.9871,  0.9997,  1.0000,  0.9994,\n",
       "          0.9985, -0.8971, -0.9983, -0.9988, -0.9998, -1.0000, -0.9436,  0.8573,\n",
       "          0.8702, -0.9999, -0.1539,  0.8623,  0.9999,  0.9831, -0.9989,  0.1691,\n",
       "         -0.9991, -0.9979,  0.9996, -0.6043, -0.9983,  0.9981, -0.8581,  0.9999,\n",
       "         -0.5752,  0.6706,  0.9543,  0.8837,  0.9589, -0.9998,  0.9420,  0.9999,\n",
       "          0.9059, -0.9999,  0.5423, -0.9606, -0.9995, -0.3854,  0.9508,  0.9919,\n",
       "         -0.9999, -0.3259, -0.9981,  0.6509,  0.8350,  0.9986,  0.9989,  0.9166,\n",
       "          0.8066,  0.9660, -0.5929,  0.9998,  0.3175, -0.9992,  0.9979, -0.8132,\n",
       "          0.8596, -0.9999,  0.9978,  0.9110,  0.9997,  0.7179,  0.6206, -0.9530,\n",
       "         -0.8125,  0.9976,  1.0000, -0.8105,  0.8461, -0.9998, -0.9975, -0.9964,\n",
       "         -0.4927, -0.8135, -0.9825, -0.9985,  0.3454,  0.7253,  1.0000,  0.9999,\n",
       "          0.9960, -0.9655, -0.9534,  0.9902,  0.9251,  0.9972, -0.8556, -1.0000,\n",
       "         -0.9819, -0.9975,  0.9826, -0.8732, -0.6612, -0.8650,  0.4159,  0.9283,\n",
       "         -0.9991, -0.9931, -0.9776,  0.5355,  1.0000, -0.9482,  0.9965, -0.9949,\n",
       "          0.8500,  0.7051,  0.7652,  0.9981, -0.5609, -0.2956, -0.9747,  0.6955,\n",
       "          0.9814,  0.9947, -0.7505,  0.8498,  0.9972, -0.9516,  0.9951,  0.7191,\n",
       "          0.9432,  0.9820,  0.9999,  0.7496,  0.9676,  0.8387,  0.9998,  0.9999,\n",
       "         -0.9340,  0.4001,  0.8900, -0.8397, -0.7286,  0.8821,  1.0000,  0.7674,\n",
       "         -0.8038, -0.9998,  0.9962,  0.8770,  1.0000, -0.6155,  0.9985, -0.5773,\n",
       "          0.9096,  0.8876,  0.6927,  0.5864,  0.8171,  0.9065,  0.9995, -0.9992,\n",
       "         -1.0000, -1.0000,  1.0000,  0.9992, -0.6457, -1.0000,  0.9983, -0.0398,\n",
       "          0.9840,  0.9971, -0.2770, -0.9750,  0.8472, -0.9980,  0.5689,  0.8958,\n",
       "          0.5679,  0.5567,  0.9987, -0.9981, -0.2194,  1.0000, -0.3332,  0.9996,\n",
       "          0.8554, -0.9889,  0.9897, -0.9911, -0.9990, -0.9437,  0.9991,  0.9989,\n",
       "         -0.7026,  0.0942,  0.9992, -0.9946,  0.9999, -0.9999,  0.9860, -0.9976,\n",
       "          0.9996, -0.9690, -0.9938, -0.4258,  0.5506,  0.9679, -0.9178,  0.9998,\n",
       "         -0.6767, -0.5283, -0.3774, -0.9763, -0.9968, -0.9955,  0.6699, -0.9985,\n",
       "          0.9638,  0.6016, -0.8549, -0.9228, -0.9979,  0.9997, -0.3943, -0.9741,\n",
       "          0.9996, -0.9930, -1.0000,  0.8686, -0.9921,  0.4578,  0.9806,  0.4823,\n",
       "          0.0960, -1.0000,  0.5924,  0.9995, -0.9992, -0.6825, -0.9735, -0.9967,\n",
       "          0.8306,  0.9795,  0.7845,  0.5389,  0.8801,  0.1180,  0.8865, -0.0784,\n",
       "          0.0814, -0.9997, -0.9562, -0.9224, -0.9991, -0.9995, -1.0000,  1.0000,\n",
       "          0.9986,  0.9997,  0.0904, -0.6619,  0.8108,  0.9920, -0.9993, -0.9178,\n",
       "          0.4926,  0.9487, -0.7893, -0.9989, -0.7475, -0.9999, -0.1785,  0.1690,\n",
       "         -0.9641, -0.5687,  0.9998,  0.9998, -0.9962, -0.9942, -0.9992, -0.9961,\n",
       "          0.9997,  0.9968,  0.9978,  0.0334, -0.7857,  0.9758, -0.6620, -0.0181,\n",
       "         -0.9963, -0.9916, -0.9974,  0.7746, -0.9918, -0.9985,  0.9698,  0.9995,\n",
       "          0.2345, -0.9999, -0.7451,  0.9987,  0.9992,  1.0000,  0.2788,  0.9997,\n",
       "         -0.9968,  0.9946, -0.9999,  0.9999, -0.9998,  0.9999,  1.0000,  0.8541,\n",
       "          0.9997, -0.9983,  0.9945, -0.1585, -0.5829,  0.9214, -0.9682, -0.9966,\n",
       "          0.6579,  0.9979, -0.8675,  1.0000,  0.8327,  0.8039,  0.5584,  0.7912,\n",
       "          0.9144, -0.4889, -0.9997,  0.9087,  0.9998,  0.9922,  1.0000,  0.9705,\n",
       "          0.9997, -0.9820, -0.9968,  0.6097, -0.6067, -0.0341, -0.9977,  0.9997,\n",
       "          1.0000, -0.9995, -0.6181,  0.5720,  0.5846,  0.9991,  0.9930,  0.9959,\n",
       "          0.8087, -0.0906,  0.9983, -0.9997,  0.5549, -0.8945, -0.1764,  0.9995,\n",
       "         -0.5407,  0.9985, -0.9921,  0.9999, -0.7301,  0.4208,  0.9982,  0.9969,\n",
       "         -0.9949,  0.9999,  0.5681, -0.9932, -0.9343, -0.9996, -0.9914,  0.8119]],\n",
       "       grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30, 768])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state=outputs[0]\n",
    "outputs[0].shape # last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1].shape # pooler_output # 整个句子的Pooler output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_embeddings=last_hidden_state[:,0] # 第一个字符CLS的embedding表示\n",
    "last_hidden_state[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}