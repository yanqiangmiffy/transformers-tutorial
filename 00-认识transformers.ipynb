{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig,AutoModel,AutoTokenizer,AdamW,get_linear_schedule_with_warmup,logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset,SequentialSampler,RandomSampler,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预训练模型名称\n",
    "MODEL_NAME=\"bert-base-chinese\"\n",
    "# MODEL_NAME=\"roberta-large\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  预训练模型配置\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.6.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer\n",
    "\n",
    "参考文档：https://huggingface.co/transformers/v4.6.0/main_classes/tokenizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='bert-base-chinese', vocab_size=21128, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一些特殊符号：['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 102, 0, 101, 103]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词汇表大小\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将文本转为词汇表id\n",
    "\n",
    "- 方法1\n",
    "```\n",
    "    def encode(\n",
    "        self,\n",
    "        text: Union[TextInput, PreTokenizedInput, EncodedInput],\n",
    "        text_pair: Optional[Union[TextInput, PreTokenizedInput, EncodedInput]] = None,\n",
    "        add_special_tokens: bool = True,\n",
    "        padding: Union[bool, str, PaddingStrategy] = False,\n",
    "        truncation: Union[bool, str, TruncationStrategy] = False,\n",
    "        max_length: Optional[int] = None,\n",
    "        stride: int = 0,\n",
    "        return_tensors: Optional[Union[str, TensorType]] = None,\n",
    "        **kwargs\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\n",
    "\n",
    "        Same as doing ``self.convert_tokens_to_ids(self.tokenize(text))``.\n",
    "\n",
    "        Args:\n",
    "            text (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`):\n",
    "                The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the\n",
    "                ``tokenize`` method) or a list of integers (tokenized string ids using the ``convert_tokens_to_ids``\n",
    "                method).\n",
    "            text_pair (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`, `optional`):\n",
    "                Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
    "                the ``tokenize`` method) or a list of integers (tokenized string ids using the\n",
    "                ``convert_tokens_to_ids`` method).\n",
    "        \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2769, 1762, 1266, 776, 2339, 868, 102]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"我在北京工作\"\n",
    "token_ids=tokenizer.encode(text)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '我', '在', '北', '京', '工', '作', '[SEP]']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将id转为原始字符\n",
    "tokenizer.convert_ids_to_tokens(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "padding的模式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "padding (:obj:`bool`, :obj:`str` or :class:`~transformers.file_utils.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "                 Select a strategy to pad the returned sequences (according to the model's padding side and padding\n",
    "                 index) among:\n",
    "\n",
    "                * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
    "                  single sequence if provided).\n",
    "                * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "                  maximum acceptable input length for the model if that argument is not provided.\n",
    "                * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "                  different lengths).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2769, 1762, 1266, 776, 2339, 868, 102]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加入参数\n",
    "token_ids=tokenizer.encode(text,padding=True,max_length=30,add_special_tokens=True)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2769,\n",
       " 1762,\n",
       " 1266,\n",
       " 776,\n",
       " 2339,\n",
       " 868,\n",
       " 102,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加入参数\n",
    "token_ids=tokenizer.encode(text,padding=\"max_length\",max_length=30,add_special_tokens=True)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 2769, 1762, 1266,  776, 2339,  868,  102,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids=tokenizer.encode(text,padding=\"max_length\",max_length=30,add_special_tokens=True,return_tensors='pt')\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 方法2 encode_plus\n",
    "```\n",
    "def encode_plus(\n",
    "        self,\n",
    "        text: Union[TextInput, PreTokenizedInput, EncodedInput],\n",
    "        text_pair: Optional[Union[TextInput, PreTokenizedInput, EncodedInput]] = None,\n",
    "        add_special_tokens: bool = True,\n",
    "        padding: Union[bool, str, PaddingStrategy] = False,\n",
    "        truncation: Union[bool, str, TruncationStrategy] = False,\n",
    "        max_length: Optional[int] = None,\n",
    "        stride: int = 0,\n",
    "        is_split_into_words: bool = False,\n",
    "        pad_to_multiple_of: Optional[int] = None,\n",
    "        return_tensors: Optional[Union[str, TensorType]] = None,\n",
    "        return_token_type_ids: Optional[bool] = None,\n",
    "        return_attention_mask: Optional[bool] = None,\n",
    "        return_overflowing_tokens: bool = False,\n",
    "        return_special_tokens_mask: bool = False,\n",
    "        return_offsets_mapping: bool = False,\n",
    "        return_length: bool = False,\n",
    "        verbose: bool = True,\n",
    "        **kwargs\n",
    "    ) -> BatchEncoding:\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2769, 1762, 1266,  776, 2339,  868,  102,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids=tokenizer.encode_plus(\n",
    "    text,padding=\"max_length\",\n",
    "    max_length=30,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors='pt',\n",
    "    return_token_type_ids=True,\n",
    "    return_attention_mask=True\n",
    ")\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model=AutoModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs=model(token_ids['input_ids'],token_ids['token_type_ids'])\n",
    "outputs=model(token_ids['input_ids'],token_ids['attention_mask'])\n",
    "\n",
    "# outputs=model(token_ids['input_ids'],token_ids['attention_mask'],token_ids['token_type_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.2670, -0.0858,  0.2122,  ..., -0.0070,  0.9425, -0.3466],\n",
       "         [ 0.5193, -0.3700,  0.4482,  ..., -1.0237,  0.7864, -0.1775],\n",
       "         [-0.1792, -0.7018,  1.0653,  ..., -0.3034,  1.0692,  0.0429],\n",
       "         ...,\n",
       "         [-0.2591,  0.0598,  0.3403,  ..., -0.1995,  0.1566, -0.3007],\n",
       "         [-0.2168,  0.0471,  0.3638,  ..., -0.2013,  0.2269, -0.3189],\n",
       "         [-0.2386, -0.0272,  0.2252,  ..., -0.0456, -0.0596, -0.2200]]],\n",
       "       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[ 0.9986,  0.9999,  0.9988,  0.9545, -0.6417,  0.5586,  0.3451,  0.6832,\n",
       "          0.9936, -0.9965,  1.0000,  0.9999,  0.0969, -0.9015,  0.9994, -0.9996,\n",
       "         -0.0634,  1.0000,  0.9828,  0.5460,  0.9992, -1.0000, -0.9602, -0.9486,\n",
       "         -0.8842,  0.9878,  0.9769,  0.0949, -0.9995,  0.9895,  0.9659,  0.9994,\n",
       "          0.9980, -0.9999, -0.9976,  0.5098, -0.7977,  0.9948, -0.7914, -0.9849,\n",
       "         -0.9965, -0.5981,  0.3857, -0.9975, -0.9579,  0.4100, -1.0000, -1.0000,\n",
       "          0.8568,  0.9991, -0.1765, -1.0000,  0.9296, -0.9318,  0.8056,  0.9725,\n",
       "         -0.9998,  0.8912,  1.0000,  0.3592,  0.9997, -0.7306, -0.6022, -0.9998,\n",
       "          1.0000, -0.9999, -0.9528,  0.1521,  0.9995,  1.0000, -0.9858,  0.4340,\n",
       "          1.0000,  0.9561, -0.7498,  0.9997, -0.9917,  0.6525, -1.0000, -0.5290,\n",
       "          1.0000,  0.9993, -0.9347,  0.8421, -0.9891, -0.9999, -0.9998,  0.9999,\n",
       "         -0.5783,  0.8760,  0.9945, -0.9977, -1.0000,  0.9980, -0.9983, -0.9987,\n",
       "         -0.8752,  0.9981, -0.3967, -0.8975, -0.5133,  0.9742, -0.9992, -0.9991,\n",
       "          0.9994,  0.9994,  0.7277, -0.9995,  0.9999,  0.7907, -1.0000, -0.9462,\n",
       "         -1.0000,  0.2198, -0.9616,  0.9996,  0.4455, -0.3929,  0.9995, -0.9991,\n",
       "          0.7031, -0.9999, -0.7935, -0.9974,  0.9999,  0.9999,  0.9985, -0.9997,\n",
       "          0.9998,  1.0000,  0.9194,  0.9896, -0.9930,  0.9952, -0.1052, -0.9834,\n",
       "          0.7176, -0.9664,  1.0000,  0.9652,  0.9823, -0.9853,  0.9957, -0.9980,\n",
       "          0.9999, -1.0000,  0.9945, -1.0000, -0.9994,  0.9953,  0.9923,  1.0000,\n",
       "         -0.7978,  0.9999, -0.9812, -0.9999,  0.9990, -0.0079,  0.9991, -0.9999,\n",
       "          0.9872,  0.8773, -0.8599,  0.7851, -1.0000,  0.9999, -0.8774,  1.0000,\n",
       "          0.9998, -0.8900, -0.9732, -0.9988,  0.9746, -0.9995, -0.9984,  0.9864,\n",
       "         -0.3062,  0.9885, -0.9927, -0.9211,  0.7024, -0.8854, -0.9998,  0.9979,\n",
       "         -0.1070, -0.2068,  0.6250,  0.8880,  0.9973,  0.9898, -0.7060,  0.9999,\n",
       "         -0.0964,  0.9962,  0.9989, -0.0794, -0.7561, -0.9706, -1.0000,  0.3083,\n",
       "          0.9999, -0.7450, -0.9987,  0.9098, -1.0000,  0.9353, -0.2246,  0.5185,\n",
       "         -0.9900, -0.9999,  0.9999, -0.9718, -0.9958,  0.6067, -0.9118,  0.3253,\n",
       "         -1.0000,  0.9202,  0.9909, -0.8688,  0.5344, -0.7166, -0.9953,  0.9309,\n",
       "         -0.8199,  0.9348,  0.9977,  1.0000,  0.9804, -0.7467, -0.9335,  1.0000,\n",
       "          0.5077, -1.0000,  0.5815, -0.7935, -0.7349,  0.9998, -0.9990,  0.9095,\n",
       "          1.0000,  0.9921,  1.0000, -0.2125, -0.9989, -0.9970,  1.0000,  0.9978,\n",
       "          0.9998, -0.9985, -0.9991,  0.6060, -0.1385, -1.0000, -0.9962, -0.8801,\n",
       "          0.9911,  1.0000,  0.2897, -0.9998, -0.2624, -0.9993,  1.0000, -0.8487,\n",
       "          1.0000,  0.9556, -0.8725, -0.9962,  0.8722, -0.5077, -0.9997, -0.2779,\n",
       "         -0.9996, -0.9924, -0.9999,  0.9055, -0.9990, -1.0000,  0.8632,  0.9999,\n",
       "          0.9105, -0.9998,  0.9996,  0.9957, -0.9611, -0.9996,  0.9823, -1.0000,\n",
       "          1.0000, -0.9969,  0.6207, -0.0030, -0.9880, -0.8604,  0.9991,  0.9997,\n",
       "         -0.9974, -0.9256, -0.8272, -0.9999, -0.7311,  0.8521,  0.0231,  0.7647,\n",
       "         -0.9838, -0.9336,  0.8415, -0.9954, -0.9999, -0.9192,  1.0000, -0.4956,\n",
       "          1.0000,  0.4524,  1.0000,  0.9832, -0.9993,  0.9930,  0.8250, -0.5943,\n",
       "         -0.7908, -0.9861,  0.8129,  0.2001,  0.5161, -0.9995,  0.9997,  0.9983,\n",
       "          0.9893,  0.9763,  0.3462, -0.4559,  0.9393, -0.9982,  0.9976, -0.9996,\n",
       "         -0.7520,  0.9971,  0.9999,  0.9999,  0.7595, -0.8876,  0.9727, -0.9980,\n",
       "          0.9970, -0.9974,  0.9985, -0.9960,  0.9693, -0.7504, -0.9917,  1.0000,\n",
       "          0.9545, -0.6712,  1.0000, -0.9418,  0.9384,  0.9999,  0.9206,  0.9717,\n",
       "          0.6311,  0.9999, -0.9986, -0.9966, -0.9973, -0.9944, -0.9988, -1.0000,\n",
       "          0.4478, -0.9976, -0.9626, -0.9599,  0.5757, -0.0107, -0.7348,  0.0048,\n",
       "          0.0723,  0.8022, -0.9708,  0.2892,  0.9310, -0.9980, -0.9384, -1.0000,\n",
       "         -0.9981,  0.9888,  0.9992, -0.9997,  0.9997, -1.0000, -0.9987,  0.9901,\n",
       "          0.2053, -0.5843,  0.9998, -0.9999,  0.9686,  1.0000,  1.0000,  0.9991,\n",
       "          0.9997, -0.9751, -0.9999, -0.9994, -0.9999, -1.0000, -0.9994,  0.7674,\n",
       "          0.7939, -1.0000, -0.9327,  0.9427,  1.0000,  0.9453, -0.9987,  0.8275,\n",
       "         -0.9995, -0.9830,  0.9995, -0.6096, -0.9989,  0.9999, -0.1734,  1.0000,\n",
       "         -0.8638,  0.9956,  0.9765,  0.7885,  0.9677, -1.0000,  0.7434,  1.0000,\n",
       "          0.5149, -0.9999, -0.5679, -0.9572, -1.0000, -0.1615,  0.9307,  0.9999,\n",
       "         -0.9999, -0.6308, -0.9919,  0.3437,  0.9118,  0.9999,  0.9988,  0.8609,\n",
       "          0.3412,  0.9425,  0.1690,  0.9997,  0.4484, -0.9968,  0.9974, -0.2034,\n",
       "          0.5577, -1.0000,  0.9962,  0.4399,  0.9999,  0.9959,  0.6560, -0.9489,\n",
       "         -0.9596,  0.9954,  1.0000, -0.9612,  0.9706, -0.9990, -1.0000, -0.9989,\n",
       "         -0.0476, -0.7789, -0.9785, -0.9992,  0.8798,  0.9559,  1.0000,  0.9999,\n",
       "          0.9957, -0.7819, -0.9561,  0.9869,  0.0119,  0.9998, -0.7133, -1.0000,\n",
       "         -0.9949, -0.9999,  0.9996, -0.9068, -0.9097, -0.9300, -0.3992,  0.8845,\n",
       "         -0.9999, -0.8416, -0.9979,  0.4116,  1.0000, -0.9875,  0.9986, -0.9986,\n",
       "         -0.0395,  0.7331,  0.9024,  0.9995, -0.5490, -0.6971, -0.7122,  0.8567,\n",
       "          0.9874,  0.9989, -0.9868,  0.8329,  0.9981, -0.9835,  0.9991,  0.6488,\n",
       "          0.7209,  0.9834,  1.0000,  0.3964,  0.9979,  0.8983,  0.9999,  0.9999,\n",
       "         -0.9403,  0.6022,  0.8283, -0.8373, -0.1218,  0.9771,  0.9999,  0.6683,\n",
       "         -0.9757, -0.9997,  0.9984,  0.9961,  1.0000,  0.7415,  0.9946, -0.5225,\n",
       "          0.9588,  0.8054,  0.7780,  0.1452,  0.4877,  0.9282,  0.9990, -1.0000,\n",
       "         -1.0000, -1.0000,  1.0000,  0.9999, -0.6069, -1.0000,  0.9994, -0.6409,\n",
       "          0.9728,  0.9938,  0.4333, -0.8666,  0.9610, -0.9995, -0.0485,  0.2587,\n",
       "          0.3155,  0.7848,  0.9992, -0.9998, -0.6526,  1.0000,  0.0809,  0.9999,\n",
       "          0.4925, -0.9816,  0.9979, -0.9703, -0.9998, -0.9115,  0.9998,  0.9994,\n",
       "         -0.6118, -0.3596,  0.9993, -0.9996,  0.9999, -0.9999,  0.8994, -0.9990,\n",
       "          0.9999, -0.9854, -0.9989, -0.5286,  0.1115,  0.9979, -0.5575,  0.9999,\n",
       "         -0.7099, -0.9667, -0.4315, -0.9133, -0.9996, -0.9925,  0.1584, -0.9999,\n",
       "          0.8137, -0.6677, -0.1643, -0.9849, -0.9998,  0.9999, -0.8938, -0.9912,\n",
       "          0.9999, -0.9979, -1.0000,  0.7306, -0.9942, -0.5475,  0.9840,  0.6176,\n",
       "          0.4018, -1.0000,  0.5113,  0.9995, -0.9994, -0.9433, -0.9860, -0.9887,\n",
       "          0.2204,  0.9866,  0.9670, -0.0998,  0.3975, -0.2984,  0.8272,  0.6054,\n",
       "          0.4439, -0.9957, -0.9461, -0.9810, -0.9991, -0.9991, -0.9999,  1.0000,\n",
       "          0.9998,  0.9999,  0.7350, -0.8119,  0.7291,  0.9982, -0.9996, -0.5762,\n",
       "          0.7971,  0.9614, -0.5536, -0.9997, -0.6525, -1.0000, -0.6792, -0.2272,\n",
       "         -0.9713,  0.5998,  1.0000,  0.9999, -0.9997, -0.9987, -0.9992, -0.9950,\n",
       "          0.9997,  0.9985,  0.9994, -0.8969, -0.7800,  0.9759,  0.1705, -0.1565,\n",
       "         -0.9984, -0.9964, -0.9998,  0.7632, -0.9969, -0.9995,  0.9998,  0.9996,\n",
       "          0.6582, -0.9999, -0.8716,  0.9998,  0.9992,  1.0000,  0.9521,  0.9998,\n",
       "         -0.9946,  0.9990, -0.9999,  1.0000, -1.0000,  1.0000,  1.0000,  0.9882,\n",
       "          0.9990, -0.9772,  0.9573,  0.1751, -0.3707,  0.9621, -0.6839, -0.9851,\n",
       "          0.8741,  0.9970, -0.8459,  1.0000,  0.9007,  0.6603,  0.5746,  0.9532,\n",
       "          0.8250, -0.2215, -0.9997,  0.9931,  0.9996,  0.9992,  1.0000,  0.9771,\n",
       "          0.9999, -0.9744, -0.9995,  0.9920, -0.8009,  0.5036, -0.9990,  0.9999,\n",
       "          1.0000, -0.9989, -0.7738,  0.5874,  0.4543,  0.9999,  0.9995,  0.9998,\n",
       "          0.9350, -0.1731,  0.9999, -0.9996,  0.4741, -0.9847, -0.9183,  1.0000,\n",
       "         -0.8106,  0.9993, -0.9607,  1.0000, -0.9767,  0.9727,  0.9982,  0.9664,\n",
       "         -0.9946,  1.0000,  0.0554, -0.9968, -0.9992, -0.9921, -0.9907,  0.8621]],\n",
       "       grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state=outputs[0]\n",
    "outputs[0].shape # last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1].shape # pooler_output # 整个句子的Pooler output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_embeddings=last_hidden_state[:,0] # 第一个字符CLS的embedding表示\n",
    "last_hidden_state[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对Bert输出进行变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.6.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update({\n",
    "            'output_hidden_states':True\n",
    "            }) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model=AutoModel.from_pretrained(MODEL_NAME,config=config)\n",
    "\n",
    "outputs=model(token_ids['input_ids'],token_ids['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output', 'hidden_states'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30, 768])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['pooler_output'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs['hidden_states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30, 768])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['hidden_states'][-1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 更改1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 1, 30, 768])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_hidden_states=torch.stack(outputs.hidden_states)\n",
    "all_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30, 3072])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_over_last_layers = torch.cat(\n",
    "            (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]),-1\n",
    "        )\n",
    "cat_over_last_layers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, h_size, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(h_size, hidden_dim)\n",
    "        self.V = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, features):\n",
    "        att = torch.tanh(self.W(features))\n",
    "        score = self.V(att)\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = torch.sum(context_vector, dim=1)\n",
    "\n",
    "        return context_vector\n",
    "h_size=config.hidden_size\n",
    "head = AttentionHead(h_size*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_pooling = cat_over_last_layers[:, 0]   \n",
    "head_logits = head(cat_over_last_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3072])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_pooling.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3072])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6144])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([head_logits, cls_pooling], -1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 更改2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, h_size, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(h_size, hidden_dim)\n",
    "        self.V = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, features):\n",
    "        att = torch.tanh(self.W(features))\n",
    "        score = self.V(att)\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = torch.sum(context_vector, dim=1)\n",
    "\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPoolingHead(nn.Module):\n",
    "    def __init__(self, h_size, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(h_size, hidden_dim)\n",
    "        self.V = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, last_hidden_state,attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLRPModel(nn.Module):\n",
    "    def __init__(self,transformer,config):\n",
    "        super(CLRPModel,self).__init__()\n",
    "        self.h_size = config.hidden_size\n",
    "        self.transformer = transformer\n",
    "        self.attention_head = AttentionHead(self.h_size)\n",
    "        self.mean_pooling_head = MeanPoolingHead(self.h_size)\n",
    "\n",
    "        self.linear = nn.Linear(self.h_size, 1)\n",
    "              \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        transformer_out = self.transformer(input_ids, attention_mask)\n",
    "        att_out = self.attention_head(transformer_out.last_hidden_state)\n",
    "        mp_out = self.mean_pooling_head(transformer_out.last_hidden_state,attention_mask)\n",
    "\n",
    "        x = self.linear(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask=token_ids['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_head = AttentionHead(config.hidden_size)\n",
    "mean_pooling_head = MeanPoolingHead(config.hidden_size)\n",
    "\n",
    "att_out = attention_head(outputs.last_hidden_state)\n",
    "mp_out = mean_pooling_head(outputs.last_hidden_state,attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1536])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([att_out, mp_out], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
